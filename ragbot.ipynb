{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Brought to You by AI Robotics Club \n",
    "\n",
    "**AI Robotics Club** is a student-led organization at **The Ohio State University** that brings together students from all disciplines to explore the exciting world of artificial intelligence and robotics. Whether you're studying engineering, business, design, or any other field, AI Robotics Club welcomes everyone passionate about innovation and collaboration in AI. \n",
    "\n",
    "### Why Join AI Robotics Club?\n",
    "- Open to **all majors and disciplines**—no prior experience required.  \n",
    "- Provides opportunities to work on cutting-edge AI and robotics projects.  \n",
    "- Offers workshops, speaker, and networking events to build skills and connections - we've had events featuring top OSU CSE professors, Accenture, Caterpillar, and JPMorganChase.\n",
    "\n",
    "### Connect with AI Robotics Club on Instagram:  \n",
    "Follow **@osu.airc** on Instagram to stay updated on events, workshops, and project showcases.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn About Retrieval-Augmented Generation (RAG) and Workshop Purpose\n",
    "\n",
    "### What is RAG?  \n",
    "**Retrieval-Augmented Generation (RAG)** is a powerful AI framework that enhances a language model's responses by retrieving relevant information from external documents. It ensures that AI provides accurate, context-aware answers based on specific sources.\n",
    "\n",
    "### Benefit Over ChatGPT?\n",
    "ChatGPT can forget details or make things up (we call that 'hallucinating'). RAG changes the game by connecting the AI to an actual document or set of documents. It retrieves relevant chunks of text and then uses that context to generate accurate, grounded answers. So instead of guessing, the chatbot is pulling directly from your resume, a report, or any file you give it. It’s way more reliable and useful when you need the AI to answer questions based on specific, real-world information that it otherwise wouldn’t know. (Source: ChatGPT)\n",
    "\n",
    "### Purpose of This Workshop  \n",
    "The goal of this workshop is to build a chatbot that leverages RAG to answer questions based on a document you provide. For demonstration purposes, we’ll use **my resume and experiences** to create a chatbot capable of providing tailored responses to user queries.\n",
    "\n",
    "### Recommended Video to Learn More  \n",
    "Here’s a quick video explaining RAG and its real-world applications:  \n",
    "[IBM: What is Retrieval-Augmented Generation (RAG)?](https://www.youtube.com/watch?v=T-D1OfcDW1M)\n",
    "\n",
    "By the end of this workshop, you’ll know how to:\n",
    "- Process a document (e.g., resume, reports) for RAG.\n",
    "- Set up a chatbot using tools like **LangChain**, **Ollama**, and **Gradio**.\n",
    "- Interact with the chatbot to get accurate, document-based answers.\n",
    "\n",
    "This isn’t just about building something cool—RAG is becoming a vital approach in making AI more accurate, transparent, and trustworthy by anchoring answers in real data. Whether you're new to AI or already experienced, this is a chance to build something practical and powerful—welcome aboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Ollama?\n",
    "\n",
    "**Ollama:**\n",
    "  Ollama is a local AI platform that runs large language models (LLMs) on your machine, ensuring privacy and offline capabilities. It’s ideal for tasks like chatbots, document analysis, and Retrieval-Augmented Generation (RAG).\n",
    "\n",
    "**Llama 3.1:**\n",
    "  Llama 3.1 is a cutting-edge language model from Meta AI, designed for tasks like summarization, question answering, and text generation. It integrates with Ollama for secure and efficient AI applications.\n",
    "\n",
    "# Installing and Setting Up Ollama with Llama 3.1\n",
    "Follow these steps to install **Ollama**, start the Ollama server, and download the **Llama 3.1** model.\n",
    "\n",
    "### 1. Install Ollama\n",
    "Ollama is a local language model interface for running LLMs on your machine.\n",
    "#### Installation Steps:\n",
    "- **Linux, Windows, and macOS:**:  \n",
    "  Refer to the official [Ollama installation guide](https://ollama.ai/) for platform-specific instructions.\n",
    "### 2. Start the Ollama Server\n",
    "Once Ollama is installed, run:\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "With the model installed and the server running, you can now integrate Llama 3.2 into your project for tasks such as Retrieval-Augmented Generation (RAG) or other AI-powered applications.\n",
    "\n",
    "Now, you can run an LLM locally on your computer. You should see the Ollama server pop up; if you do, try saying \"hi\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages\n",
    "\n",
    "### Explanation of Libraries Installed\n",
    "\n",
    "The following libraries are essential for the project:\n",
    "\n",
    "- **PyPDF2**: For extracting and processing text from PDF files.  \n",
    "- **pandas**: To efficiently manipulate and analyze tabular data.  \n",
    "- **langchain-ollama**: For integrating the Ollama LLM into the RAG framework.  \n",
    "- **chromadb**: To create and manage the vector store for document embeddings.  \n",
    "- **gradio**: For building a simple, user-friendly web interface.\n",
    "\n",
    "In order to do this, we will first get into a virtual environment. It is best practice to install Python libraries in virtual environments for dependency isolation, reproducibility, and a cleaner environment overall. In order to get into a virtual environment, open up a new terminal, navigate to desired folder, and run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because for some people, only 'python3' works.\n",
    "\n",
    "Then, to activate the virtual environment, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, inside your virtual environment, run following command, which installs the essential libraries needed for the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf2 pandas langchain-ollama chromadb gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Llama 3.2 with LangChain Ollama\n",
    "\n",
    "The following Python code demonstrates how to initialize the **Llama 3.2** language model and its embedding model using **LangChain Ollama**. This setup is essential for integrating advanced natural language processing capabilities into your project.\n",
    "\n",
    "Create a new file, name it llm_setup.py, paste the code seen below, and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python llm_setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 llm_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM \n",
    "from langchain_ollama import OllamaEmbeddings \n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OllamaLLM(model=\"llama3.2\", base_url=\"http://127.0.0.1:11434\")\n",
    "embed_model = OllamaEmbeddings(model=\"llama3.2\", base_url=\"http://127.0.0.1:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Text from a PDF Using PyPDF2\n",
    "\n",
    "The following Python code reads and extracts text from a PDF file, allowing you to display the contents of your resume or any document of your choice. Replace the `file_path` with the path to your specific PDF file.\n",
    "\n",
    "Create a new file named readpdf.py, paste the code seen below, and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aum Suthar is a highly skilled and driven Materials Science and Engineering student at The \n",
      "Ohio State University , expected to graduate in May 2026. His technical expertise spans a wide \n",
      "range of programming languages, including Java, JavaScript, Python, C#, C++, and CSS, as well \n",
      "as frameworks such as TensorFlow, PyTorch, sklearn, and Robot Operating System (ROS). Aum  \n",
      "is proficient in advanced CAD tools like SOLIDWORKS, Creo, Inventor, Fusion 360, and \n",
      "COMSOL, and he is experienced in data analysis using Minitab, Excel, MATLAB, SQL, and \n",
      "Tableau. His laboratory capabilities include SEM, EDS, XRD, XRF, optical microscopy,  \n",
      "metallography, and mechanical testing techniques like Rockwell hardness and tensile testing. \n",
      "Additionally, Aum has hands -on experience with prototyping and instruments such as NVIDIA \n",
      "Jetson, OnLogic, additive manufacturing, and soldering. He is adept in L inux, Ansys GRANTA, \n",
      "and Adobe Creative Suite tools, showcasing a robust and versatile skill set.  \n",
      " \n",
      "Aum’s professional journey is marked by impactful internships and co -op experiences. As a \n",
      "Parallel Co -Op at Caterpillar  in the Manufacturing Technology and Solutions division, Aum \n",
      "secured $30,000 in funding by presenting to stakeholders innovative strategies for optimizing \n",
      "manufacturing and defect detection. He developed a machine learning model using \n",
      "TensorFlow, which ach ieved an impressive 85% accuracy in detecting weld defects. \n",
      "Furthermore, he designed and prototyped a wearable defect notification system  for welders, \n",
      "leveraging an ESP32 microcontroller, USB -C interface, and 3D printing to enhance real -time \n",
      "feedback and operational efficiency. He also conducted Technology Readiness Level (TRL) \n",
      "reviews to ensure that new technologies met stakeholder require ments, security, and safety \n",
      "standards.  \n",
      " \n",
      "In a prior role as a Corporate Engineering Intern  at Caterpillar , Aum performed factory \n",
      "audits to identify opportunities for design and process improvements, enhancing asset \n",
      "connectivity and driving operational efficiency. He integrated a CNC lathe with a gantry robot \n",
      "to streamline material transport, researched Laser -Induced Breakdown Spectroscopy (LIBS) \n",
      "for alloy composition analysis, and conducted Rockwell hardness tests on ground -engaging \n",
      "tools to assess wear resistance. Aum also developed a Python Open CV application to integrate \n",
      "machine vision systems, enabling scalable defect detection across diverse factory settings. By \n",
      "leveraging 3D scanning metrology and Geometric Dimensioning and Tolerancing (GD&T), he \n",
      "modeled sputtering targets, saving $30,000 ann ually in production costs. Additionally, he \n",
      "improved additive manufacturing throughput at Caterpillar’s Makerspace by designing a \n",
      "stackable, ventilated enclosure to enhance part bed adhesion.  \n",
      " \n",
      "Aum’s research experience includes a Summer Research Internship  at Argonne National \n",
      "Laboratory  in the X -Ray Sciences Division, where he implemented fully automated high -\n",
      "throughput experimentation using Universal Robot UR3e robotic arms, achieving over three \n",
      "uninterrupted operational cycles. He designed innovative sample shipment and preparation \n",
      "accessories in SOLIDWORKS, improving operational consistency and usability for beamline \n",
      "users. He also developed a scalable automation algorithm in Python, integrate d with URCap \n",
      "and instrument controls, to support future design modifications. His microcontroller -based \n",
      "modular robotic pipetting solution enhanced precision in liquid handling, saving over $4,500 \n",
      "annually. Additionally, Aum conducted failure analysis on a dditively manufactured materials, \n",
      "optimizing robot end -effector designs for durability and precision.  \n",
      " \n",
      "Earlier, as a Volunteer Student Researcher  at Argonne National Laboratory , Aum \n",
      "developed the first self -guided AI robot for 3D thermal and spatial mapping in high -radiation \n",
      "DOE facilities. He utilized FLIR Lepton infrared cameras and DHT22 sensors for climate \n",
      "monitoring, improving safety during high -radiation operations. His work was presented and \n",
      "published at the 2020 International Conference on Synchrotron Radiation Equipme nt, where \n",
      "he introduced an AI -driven robotic system for monitoring particle -accelerating facilities.  \n",
      " \n",
      "Aum has demonstrated leadership through various extracurricular activities. He founded and \n",
      "serves as President of the Artificial Intelligence Robotics Club (AIRC) , a non -profit \n",
      "organization with over 100 members. He leads a team developing a 1/10th scale autonomous \n",
      "vehicle, guiding them in LiDAR, machine vision, and ROS integration. As a Recruitment \n",
      "Coordinator and Ambassador  for Leading Expeditions  at the Fisher College of Business, \n",
      "Aum improved the applicant -to-participant conversion rate by optimizing ou treach strategies \n",
      "and facilitated leadership training sessions for participants in high -stakes environments. \n",
      "Additionally, as Co -Owner and Director of mySTEMbuddy , a 501(c)(3) organization, he \n",
      "mentors robotics teams, teaches STEM courses, and raises awareness about STEM education \n",
      "through public campaigns and speeches.  \n",
      " \n",
      "Aum has also undertaken significant projects, including PhaseWise , an AI -powered chatbot for \n",
      "materials science students. This project leverages Retrieval -Augmented Generation (RAG) \n",
      "using FAISS and GPT -3.5 to provide textbook -based answers. He developed an automated \n",
      "pipeline for textbook PDF processing using LangChain an d SentenceTransformers. Another \n",
      "project involved designing a custom thermomechanical process to optimize the performance of \n",
      "6022 T6 aluminum alloy, earning a 4th place finish in a competitive  materials design \n",
      "competition. Aum has also contributed to the Society of Manufacturing Engineers Digital \n",
      "Manufacturing Challenge with an innovative additive manufacturing food storage solution, \n",
      "achieving runner -up status.  \n",
      " \n",
      "Recognized for his academic and professional excellence, Aum has received numerous awards, \n",
      "including the College of Engineering Dean’s List, the Buckeye Scholarship, and multiple \n",
      "scholarships for his contributions to materials science and engineering. His diverse experience, \n",
      "technical acumen, and leadership capabilities make him a valuable contributor to cutting -edge \n",
      "research and development initiatives.  \n",
      " \n",
      " \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read and extract text from a PDF file.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "file_path = '/Users/aumsuthar/Documents/Resume Rag Bot/Aum Suthar Resume Summary.pdf'  # Replace with your PDF file path\n",
    "pdf_content = read_pdf(file_path)\n",
    "print(pdf_content, \"\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Text and Creating a Vector Store for Retrieval\n",
    "\n",
    "The following code splits the PDF content into manageable chunks and stores them in a vector store, enabling efficient retrieval for Retrieval-Augmented Generation (RAG) workflows. Make a new file called vector.py and run this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import PyPDF2\n",
    "from llm_setup import embed_model\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    \"\"\"Read and extract text from a PDF file.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(file_path, 'rb') as pdf_file:\n",
    "        reader = PyPDF2.PdfReader(pdf_file)\n",
    "        for page in reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "# Load the PDF file\n",
    "file_path = '/Users/dylanjian/Desktop/Dylan_Jian_Resume.pdf'  # Replace with your PDF file path\n",
    "pdf_content = read_pdf(file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=128)\n",
    "chunks = text_splitter.split_text(pdf_content)\n",
    "vector_store = Chroma.from_texts(chunks, embed_model)\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up the Retrieval Chain for Question-Answering, Then Testing with Query\n",
    "\n",
    "The following code initializes the **Retrieval-Augmented Generation (RAG)** pipeline by creating a retrieval chain and defining how retrieved documents are processed and used for generating responses. Make a new file called ques_setup.py for this code and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of Aum based on the context:\n",
      "\n",
      "Aum appears to be a multi-faceted individual with expertise in various fields. He has experience as an engineer and researcher, having utilized Ansys GRANTA to analyze materials and designed a thermomechanical processing plan for a cast aluminum alloy. Additionally, he has skills in project management and team leadership, leading a team developing an autonomous vehicle.\n",
      "\n",
      "Aum also has entrepreneurial and business skills, serving as a Recruitment Coordinator and Ambassador at the Fisher College of Business, where he improved applicant-to-participant conversion rates and facilitated leadership training sessions.\n",
      "\n",
      "Outside of work, Aum is also a creative problem-solver who prototyped a solution to address lawn damage using 3D printed parts and various power tools. He has received several awards and distinctions for his academic achievements, including being on the College of Engineering Dean's List and receiving scholarships from reputable organizations.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from llm_setup import llm\n",
    "from vector import retriever\n",
    "\n",
    "# First create the combine_docs_chain\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "\n",
    "# Then create the retrieval chain using the combine_docs_chain\n",
    "chain = create_retrieval_chain(combine_docs_chain=combine_docs_chain, retriever=retriever)\n",
    "response = chain.invoke({\"input\": \"Give me a summary of Aum\"})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Gradio Simple Interface for the Chatbot\n",
    "\n",
    "The following code sets up a web-based interface using **Gradio** to interact with a chatbot that retrieves and summarizes information from the document you put in earlier. This interface allows users to ask questions about the resume and receive relevant responses.\n",
    "Paste the code below into a new doc, grad_setup.py and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from ques_setup import chain as retrieval_chain\n",
    "\n",
    "llm = ChatOllama(model=\"llama2\", base_url=\"http://127.0.0.1:11434\")\n",
    "embed_model = OllamaEmbeddings(model=\"llama2\", base_url=\"http://127.0.0.1:11434\")\n",
    "\n",
    "def query_pdf(prompt):\n",
    "    try:\n",
    "        response = retrieval_chain.invoke({\"input\": prompt})\n",
    "        return response.get('answer', 'No relevant information found.')\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Chatbot For Your Document\")\n",
    "    with gr.Row():\n",
    "        input_box = gr.Textbox(\n",
    "            label=\"Enter your question\",\n",
    "            placeholder=\"Type your question here...\"\n",
    "        )\n",
    "        output_box = gr.Textbox(\n",
    "            label=\"Response from Chatbot\",\n",
    "            interactive=False\n",
    "        )\n",
    "    submit_button = gr.Button(\"Submit\")\n",
    "    \n",
    "    # Link input, button, and output\n",
    "    submit_button.click(query_pdf, inputs=input_box, outputs=output_box)\n",
    "\n",
    "# Launch the interface\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
